# European Rover Challenge 2025 - Remote formula rules

## What is the ERC Remote Formula and what is it not?

The European Rover Challenge (ERC) is an integrated programme working towards technological developments with space exploration utilised as the leading theme. This year it comes in two variants - an On-site, and a Remote Formula. This document only specifies the rules for the Remote Formula - which may reference the On-site Formula or its rules. Due to the vast differences between the two formulas, both competitions are treated as totally separate events - with separate registration, rules, scoreboards, judging criteria and so on.

Unless specified otherwise, all rules and guidelines in this document apply only to the Remote Formula.

The main goal for this year's remote Competition is to experiment with types of tasks that are best suited for the remote formula. This means that some difficulties are to be expected and the rules are subject to changes during the Competition. The main goal is to learn from the experience - both as the Organizer, the Operator and Competitor.

## Organizer, Operators and use of materials

[roverchallenge.eu](https://roverchallenge.eu) is the main website for the European Rover Challenge and materials (or links) to materials for both formulas will be published there. Updates may also be sent via the emails provided during registration.

The main Operator for the remote formula this year is Husarion. This is a change from previous Remote Formulas and also the On-site Formula. Please do not reference any previous Remote documents, rules or materials as they will almost certainly not match the current Competition. Using materials from On-site Formula is allowed but not advised unless specified otherwise.

Both European Rover Challenge formulas are still owned and coordinated by the European Space Foundation. European Space Foundation is called the Organizer in this document.

Husarion is a polish company that specializes in industrial robotics platforms. They are known for their Husarion Panther UGV platforms - which is one of the main robots used in the competition. They are also known for their cloud platform - Husarnet - which is used for the competition's connectivity. To know more visit [husarion.com](https://husarion.com). Using support materials from Husarion is allowed and encouraged. Good starting points are Docs, Manuals and Tutorials on their website and their Github profile - [github.com/husarion](https://github.com/husarion).

## Copyrights, intellectual property and personal data

The Operator retains all copyrights to Competition materials, including but not limited to the Rulebook, Technical handbook, scoring system, Task descriptions, and any other materials provided to the Teams. The Teams retain all copyrights to the materials they provide to the Organizer and the Operator. The Teams grant both the Organizer and the Operator a non-exclusive, worldwide, royalty-free license to use the materials for the purposes of the Competition. The Teams are responsible for ensuring that they have the right to use any materials they provide to the Organizer and the Operator.

Teams may be granted the right to use the ERC logo and other materials for promotional purposes. Please contact the Organizer with a specific request to be granted permission.

Reuse of the Competition materials for other purposes is not allowed without the written consent of the Organizer and the Operator.

Copyrights and Trademarks mentioned in the Competition materials are the property of their respective owners.

Teams grant permission to both the Organizer and the Operator to use promotional materials and visuals (e.g. photos and videos), as well as any additional photos, videos, portraits, documents, interviews, and other materials resulting from participation in the Challenge (using the name of the participant or not) on all media, in any language, anywhere in the world, in any manner, for advertising and promotional and educational purposes.

Personal data and information about Team members other than their names and surnames will not be published without the prior consent of each Team member.

## Cancellation of the Competition

Organizer reserve the right to cancel the Competition at any time. The Organizer will make every effort to inform the Teams of the cancellation as soon as possible. Neither the Organizer nor the Operator are responsible for any damages or losses caused by the cancellation of the Competition.

## Awards

The exact list of Awards is to be specified later. Awards from non-Organizer entities may be allowed but must be accepted by the Organizer. Digital (PDF) certificates of participation will be Awarded in the Remote Formula this year.

Teams may be eligible for receiving physical Awards. Teams may forfeit the right to receive an Award if they choose so. All customs, legal and fees may be the responsibility of the Team - specifics will be provided in the Award announcement.

## Communication

All the communication, documents, user interfaces and other materials will be in English and should be provided (i.e. by the Teams) in English.

All communication regarding the competition should be done through the official channels. The main channel for communication is the Community Forum. The forum is set up to allow for easy communication between the Teams, the Organizer and the Operator. The Forum is also used to provide updates, answer questions and provide additional materials. In case there's a justified need to contact the Organizer or the Operator directly and privately - please use the Community Forum to obtain the contact details.

The Community Forum is located at [erc2025.husarion.com](https://erc2025.husarion.com).

Slack is **not** a valid communication channel for the Remote Formula.

## Code of conduct

The most important rule is to be excellent to each other. The Competition is meant to be fun and educational. The Organizer will not tolerate any form of harassment, discrimination, or any other form of inappropriate behavior. The Organizer and the Operator reserve the right to disqualify any team that does not adhere to the code of conduct. Feedback (especially the negative one) should be presented in a professional, constructive way. In case you don't want to share the feedback directly or publicly - please contact the Organizer or the Operator directly.

Neither Organizer nor the Operator are responsible for any content posted on the Community Forum. The Teams are responsible for their own content.

## Equipment and infrastructure usage

Operator and the Organizer will provide some infrastructure for the Competition. This includes but it's not limited to the platforms (Husarion Panther), accessories specific for each of the tasks, the Marsyard environment, and the Husarnet connectivity. The Teams will be granted remote access to such infrastructure and are subject to the Fair Use Policy. The Fair Use Policy expects that some wear and tear will occur during the Competition and that the Teams will use the infrastructure in a responsible way. The Teams are responsible for any damage or abnormal wear and tear caused by them to the infrastructure.

Any attempts to destroy or damage the infrastructure will result in disqualification. Any attempts to use the infrastructure for purposes other than the Competition will result in disqualification. Any attempts to circumvent the security policies or separation between the Teams will result in disqualification. Any attempts of unauthorized access to the infrastructure of both the Competition and other Teams will result in disqualification.

Detailed equipment description will be provided in the Technical Handbook.

## Wording

RFC 2119 keywords are used in this document.

`Teams`, `Competitors` and `Contestants` are used interchangeably and mean the same thing.

Test Drives are the events where the Teams can test their skills in the Marsyard environment. They are used to familiarize the Teams with the environment, the robots, and the tasks. They are also used to gather feedback from the Teams and the Operator. Test Drives are scored.

Final Challenge is the main task that the Teams need to complete during the Competition.

Final Challenge and Test (Drives) and called Tasks whenever both are addressed.

Scoring categories are the individual parts of the Competition that are scored. They are used to calculate the final score and are not limited to Tasks.

## Technical Handbook

Technical Handbook is a document that describes the technical details of the Competition. It includes the exact specifications of the robots used for the Competition, the environment, the connectivity, and the tasks. The Technical Handbook is a living document and may be updated during the Competition. Teams are not limited to the information provided in the Technical Handbook and are encouraged to seek knowledge and resources. Technical Handbook is the next in line authoritative source of truth after the Rulebook and the Organizer and the Operator. Any other resources are considered less authoritative.

## Dates

### Past
- 2025.02.10 - Announcement of the Competition
- 2025.02.24 - Registration opens, first version of Rulebook is published, Community Forum is set up
- 2025.03.24 - Technical Handbook for Challenge 1 is published
- ~~2025.03.31 - Technical Handbook for Challenge 2 is published~~
- 2025.04.07 - Registration closes
- 2025.04.14 - Qualification results are published
- 2025.06.03 - Connectivity Test
- 2025.07.15 - ROS Test

### Upcoming

> [!NOTE]
> Upcoming dates are still subject to change. They're published here as a ballpark estimate.

- 2025.07.29 - Payload Test
- 2025.08.12 - Challenge Test
- 2025.08.26 - Finals
- 2025.08.28 - Competition Report
- 2025.08.31 - Results are published, Award Ceremony

Additional Test Drive sessions may be organised during the Competition but are not guaranteed.

## Locations and time zones

Competition will physically take place in Kraków, Poland. Some parts of the Competition will use the same infrastructure as the On-site formula, namely the Marsyard - a special outdoor environment designed for testing rovers.

Competitors are allowed to participate in the Remote Formula from any location of their choice, **except** Kraków, Poland and provided that both they and the Organiser/Operator stay within the applicable laws and regulations in both the Teams and the Organizer's locations. This includes the laws regarding the transmission of technical data and intellectual property rights.

Any attempts to leverage the physical presence in Kraków during the remote formula will result in disqualification. Using cameras and other sensors (not provided by the Operator) to observe the Marsyard during the competition is also prohibited.

Neither the Organizer nor the Operator is responsible for providing any amenities or infrastructure for the specific Teams during the Competition. Teams are responsible for providing their own infrastructure, including but not limited to the sufficient internet connection, power supply, and safe workspace.

All the times mentioned in the Competition materials are in the Central European Summer Time (CEST) unless specified otherwise. The Teams are responsible for converting the times to their local time zones. The Operator is not responsible for any discrepancies in the time zones. The Teams are responsible for being ready for the Tasks at the specified times. Competition events will be held during the day at the Operator's time zone. The Operator is allowed to refuse a time slot outside of regular working hours in the Operator's time zone. Teams are required to be ready at a given time slot - even if the time slot is outside of the regular working hours in the Team's time zone. If, for any (i.e. legal) reason Teams are not able to participate in the Competition during the day in Operator's time zone - they should inform the Operator as soon as possible, this may result in a Team not being considered for a given Task.

## Competition limits

The Competition is limited to 25 Teams. The Tasks have a limited number of slots for Teams. The first Task will be limited to 20 Teams so there will be 5 Teams on a sort of Reserve List by design. This means that those Teams still can participate in the Competition and score points for categories like Social Excellence, Competition Report, etc. but they will not be initially invited for the first Task. They may get chosen for the following Tasks or for the first Task in case i.e. any other Teams resign.

The Operator reserve the right to change the number of Teams for a specific Task at any time.

Each Team will be assigned a time slot for completing the Task. Time slots will vary between the Tasks, but in general do not expect to be assigned more than 1 hour for a Task. The exact time slots will be published before the Task. Teams are expected to be ready for the Task at the beginning of the time slot. Teams that are not ready at the beginning of the time slot may be disqualified from the Task. Not showing up for the Task may result in disqualification from the Task. If you know in advance that you won't be able to make it to the Task - please inform the Operator as soon as possible - this is the only way to increase your chance of rescheduling to the end of the queue or be eligible for any extra sessions that may be organized.

| Milestone         | Team number limit | Time slot length |
| ----------------- | ----------------- | ---------------- |
| Initial list      | 25                |                  |
| Connectivity Test | 20                | 15 minutes       |
| ROS Test          | 20                | 15 minutes       |
| Payload Test      | 10                | 30 minutes       |
| Challenge Test    | 10                | 30 minutes       |
| Finals            | 5                 | 1 hour           |

## Team Limits

At least 75% of a Team must comprise higher education students and recent graduates (i.e. max two years after receiving master's or PhD degree): undergraduate and masters-degree level students (with no limitations) and PhD students. It is highly recommended that teams cooperate with specialists from different institutions, but students must prepare and sign all the required documentation themselves. A Team may consist of students from more than one higher education institution. An institution may also be affiliated with more than one Team. Team membership is exclusive - each person can only be a member of one Team.

A team can be comprised of at most 50 people. The team must have a supervisor who is a university employee. The Supervisor is not counted in the Team size limit. The Supervisor is responsible for the Team's compliance with the rules and regulations of the Competition. The Supervisor is not allowed to participate in the Competition directly.

Teams should be able to specify specific people responsible for typical roles, like the marketing manager, the technical manager, the team leader, etc. The roles are not limited to the mentioned ones.

Size and structure of the Teams can change during the Competition. Teams should be able to provide a list of members and their roles at any time during the Competition. Single member can have multiple roles.

Teams can compete in both Formulas but it's generally not advised due to the sheer volume of work. Teams that compete in both Formulas are subject to the same rules and limitations respective to each of the formulas as the Teams that compete in a single formula.

## Registration

Teams are subject to Registration in order to be considered for the Competition. Registration link is published on the official website. Registration is open for a limited time. The exact dates are specified in the Dates section. Registration Form specifies the requirements for the Registration, including but not limited to the GDPR forms.

## Scoring

The Operator is the sole entity responsible for scoring the Competition. All disputes regarding the scoring should be addressed to the Operator.

Competition's scoreboard is a living document that is updated after each scoring session. There can be partial points (like half, quarter and so on).

The final score is calculated based on the sum of points from each scoring category. Some categories are Task-based and final after publication (like challenge results), and some are continuous and can be updated at any time (like Social Excellence).

The Team with the highest final score wins the Competition. There can be a tie.

Jury will choose the set of Teams for each of the Tasks - based on the current high scores and Team declarations. The number of Teams for each Task is limited and is announced before the Task.

Teams can choose to skip Task and still participate in the general Scoring. Teams that skip a Task are not eligible for the Task's points. Freed slots may then be allocated to other Teams.

Disqualification eliminates a Team from further scoring. Not qualifying for a Task does not disqualify a Team from further scoring (i.e. Team can still gather points for Social Excellence, Competition Report, etc.).


| Category           | Description                                            | Maximum points | Competition phase |
| ------------------ | ------------------------------------------------------ | -------------- | ----------------- |
| Team Proposal      | Points for the Team Proposal                           | 50             | Phase 1           |
| Connectivity Test  | Points for passing the Connectivity Test               | 20             | Phase 2           |
| ROS Test           | Points for passing ROS Test                            | 50             | Phase 2           |
| Payload Test       | Points for passing the Payload Test                    | 50             | Phase 3           |
| Challenge Test     | Points for passing the Challenge Test                  | 100            | Phase 4           |
| Challenge Finals   | Points for completing the Challenge                    | 200            | Phase 5           |
| Competition Report | Points for the Competition Report                      | 80             | Phase 6           |
| Social Excellence  | Points for active participation in the Community Forum | 100            | Phase 6           |
| Jury               | Points from the Jury                                   | 100            | Phase 6           |
| **Total**          | **Sum of the categories**                              | **750**        |                   |

## Team Proposal

Team Proposal is a document submitted by each of the Teams during the Registration. It MUST contain the basic information about the Team up to the point of registration and a vague plan for the Competition. The Team Proposal is used to qualify the Teams for the Competition. The Team Proposal MUST be submitted in a PDF format. Team Proposals MAY get published by the Organizer/Operator during or after the Competition. Team Proposals published by the Organizer/Operator MAY contain suggestions and feedback from the Jury. Team Proposals MUST NOT contain any sensitive information. Team Proposals MUST NOT contain any personal information other than names and surnames (listing only the roles is also perfectly acceptable - no names are required). Team Proposals MAY contain links to external resources - like the video materials, websites, etc - provided that they won't change between the submission of the Registration Form and publication of the initial qualification list.

Team Proposals SHOULD be short and concise. The maximum length of the text in any Team Proposal is 5 A4 pages (this means that the pictures, diagrams, and other non-text elements do not count in this limit and the overall length can be more than 5 pages). The font size MUST be at least 10pt.

Team Proposal scoring overview:

| Piece of information | Description                                          | Maximum points |
| -------------------- | ---------------------------------------------------- | -------------- |
| List of skills       | List of skills and competences available to the Team | 10             |
| Team experience      | List of previous competitions and projects           | 5              |
| Team composition     | Team composition and structure and methodologies     | 5              |
| Team plan            | High-level plan for the Competition                  | 15             |
| Team equipment       | List of equipment and infrastructure                 | 5              |
| Wildcard             | Anything else that the Team wants to share           | 10             |
| **Sum**              | **Sum of the categories**                            | **50**         |

**List of skills** - list of skills that Team members have. This is not limited to technical skills. Soft skills are also important. Whenever possible should also include the level of proficiency and a number of members with a given skill. This list should also mention any skills that Team finds necessary/helpful but does **not** currently have - with a plan on how to acquire them. If you have any mentors from the university or the industry - please share what are their specialities and how they are willing to help you. Some skills that you may want to include were already mentioned in the Remote Competition announcement - but you're free to include any other that you find relevant. The most important from our point of view is the ability to use ROS (Robot Operating System) and Husarnet - our product that you'll be using to connect to the platforms.

**Team experience** - list of previous competitions and projects that the Team members participated in. This is not limited to the robotics competitions. Any experience that you find relevant should be included. This may also include the experience in the specific technologies that you plan to use during the Competition.

**Team composition/structure and organization methods** - If you have any organization structure figured out yet - please include it. This may include the roles, responsibilities, communication methods, etc. This is not limited to the technical roles - any role that you find necessary should be included. If you already figured out any methodologies that you want to use - please include them.

**Team plan** - high-level plan for the Competition. At this point we do not require any specifics but we do want you to have at least a general overview of the upcoming challenges. If you have a very specific plan for one of the Tasks - you can include it even if your plan for the other is still very vague. Descriptions for any extra axis like the marketing, documentation, infrastructure during the Tasks and during development,... - will be scored too. Having people already assigned to each of the axes will be a plus. In case a Team is competing in both Remote and On-site formula - please include the plan on how you're going to manage the two Competitions at the same time.

**Team equipment** - list of equipment and infrastructure that the Team has. This is not limited to robotics equipment. Any equipment that you find relevant should be included. This may also include the equipment that you plan to use during the Competition. Have in mind that you'll need a decent Internet connection to Poland during the Tasks and some computing power during the preparations. If you have any equipment that you plan to use but do not currently have - please share what are your plans on acquiring/renting it. If you have sponsors for the equipment - please share what they are providing you with.

**Wildcard** - you're free to write any piece of information that you think would be valuable to the Judges during the scoring of Team's Proposal. This may include the information that you think is missing from the other categories at the current time. Such pieces of information may be converted into separate categories in the next revisions of the Rulebook. Wildcard points can be negative (i.e. if the document is slightly too long, or if the information is not relevant). Judges are free to extract any pieces of information that were not scored in any other categories and assign them to the Wildcard category.

Categories listed above do not need to be explicitly specified in the document - you can form the document as you wish, as long as required pieces of information are to be found in the document.

Providing a proposal that's too long, malformed or containing false data may result in disqualification.

## Test Drives

Test Drives are the events happening every couple of weeks during the Competition. They are used to familiarize the Teams with the environment, the robots, and the Tasks. Test Drives are scored. Test Drives are also an opportunity to ask questions and provide feedback to the Operator.

During the tests the Operator will be learning too. In case of major issues impacting a limited number of Teams - Operator may either reschedule the Test Drive, provide an extra session for the affected Teams or compensate the Team with some points for a given Test. In case of major issues impacting all Teams - Operator may reschedule the Test Drive for all Teams.

### Connectivity Test

This Test Drive is focused on the connectivity between the Teams and the infrastructure. The main goal is to ensure that all Teams are able to connect to the infrastructure and that the Internet connection between the Team and the infrastructure is sufficient. The Teams are expected to connect to the infrastructure and perform basic operations. The Teams are expected to provide feedback on the connectivity and the infrastructure during the Task. There will be no ground infrastructure (specifically - neither the Marsyard nor any AR tags, etc.) available during the Connectivity Test - only the robots and the Husarnet connectivity, so do not expect to run any sophisticated workloads during the Connectivity Test. If you want to test other sensors at your own discretion - please contact the Operator first - we may provide you with some extra time or resources - but most likely you'll be moved to the end of the queue.

| Subtask                                                  | Maximum points           |
| -------------------------------------------------------- | ------------------------ |
| Basic Husarnet skills - creating account                 | 2                        |
| Basic Husarnet skills - joining groups                   | 2                        |
| Basic Husarnet skills - pinging any Husarnet device      | 2                        |
| Basic Husarnet skills - pinging the assigned robot       | 2                        |
| Using WebUI - connecting to the assigned robot via WebUI | 4                        |
| Using WebUI - viewing the camera via WebUI               | 4                        |
| Using WebUI - steering the robot around via WebUI        | 4                        |
| Completing all subtasks in the first try                 | 2                        |
| **Sum**                                                  | Sum **22**, capped to 20 |

Free Husarnet account should be enough for most setups. If you need more than that - contact Operator - we will increase your limit for the duration of the Competition.

For some of the subtasks you'll be required to provide some sort of proof that you've completed the task. This may be a screenshot, a video, a log file, screen shared on a video call, etc. The exact possibilities will be provided during the Task. Please have in mind that the proof should be provided in a timely manner - if you're not able to provide the proof in a reasonable time - the points may not be awarded. Please also have in mind that the only language that the Operator is required to understand is English - so please make sure that the content of the screen/logs are in English.

During the test be mindful of other contestants - as most likely multiple Teams will be using robots and bandwidth at the same time (which will not be the case for some other Tasks). You may or may not be notified in advance which robot you'll be using - so be prepared to connect to any of the platforms.

In case of any issues the Team must notify the Judges immediately. The Judges will provide you with the next steps - i.e  you may get moved to the end of the queue to try again.

There will be a time limit for a single take - most likely a 15 minute window. If you're not able to complete the Task in the given window, you can be moved to the end of the queue.

### ROS Test

Same as Connectivity Test but for ROS only. This time pre-made (by us) Foxglove layout is not allowed. We'd like to see any sort of custom interface/config - can be Foxglove, but a little bit more tailored for the Competition - ideally RViz or completely custom one. This time Foxglove itself won't be setup by the Organizers - if you choose to use it - you have to set it up yourself.

Goes without saying that all the sub-tasks must be completed via ROS.

| Subtask                                                                                                                   | Maximum points |
| ------------------------------------------------------------------------------------------------------------------------- | -------------- |
| Viewing any camera                                                                                                        | 3              |
| Viewing all available cameras at the same time                                                                            | 5              |
| Reading lidar data - and visualizing it on a point cloud                                                                  | 5              |
| Reading Panther sensors (i.e. the battery state)                                                                          | 3              |
| Controlling the Panther (i.e. driving around)                                                                             | 3              |
| Driving (manually) through an obstacle course                                                                             | 6              |
| Reading any ROS data from the Robot from a remote location (and **not** a local-compute VM) (Screen share does not count) | 5              |
| Reading all the sensors, at the same time from locations that will be used during the Competition                         | 10             |
| General proficiency during the Test (i.e. being on time, having contingencies ready, etc)                                 | 10             |
| **Sum**                                                                                                                   | **50**         |

A little bit of explanation here - the most significant differentiator between the Teams in the Connectivity test was the proficiency level - some felt like they've heard about the Husarnet for the first time ever (and it wasn't even the part that changed last minute), while others were able to complete all the tasks in a matter of minutes. This is why, this time, we will be rewarding not only the end result but also the way you get there (at least the one we see). Keep in mind that this test will be limited to 15 minutes per Team - so time will be a limited resource. During the previous test we were unable to accommodate second-try requests so we won't be doing them this time.

As per the "Reading all the sensors, at the same time" - distributed ROS setup is hard. Having ROS span over the Internet is even more difficult - this is why we want to encourage you to test this **early**. Test the prerequisites for the setup you want to run in the following Tasks - i.e. if you intend to run navigation stack on the local-compute - test reading lidar from the on-site compute, if you want to run object detection remotely (because of the lack of the GPU in local-compute) - test reading the camera data from your (remote) location.

### Payload Test

During this Test Drive each of the Teams will be given a 30 minute time slot for controlling a robot chosen by the Team. Ground infrastructure will have as many of the Final competition elements as possible - it won't be on Marsyard though. The main goal is to familiarize the Teams with actual robots, sensors, real world connection latency/bandwidth limitations and possibly give the ability to record materials for machine learning. Have in mind that at this point the accessories used for the Competition are not guaranteed not to be changed. The Teams are expected to provide feedback on the robots, the sensors, the connectivity, and the infrastructure during the Task. The Teams are expected to provide a report on the Task right after the Task. The Task will be scored.

| Subtask                                                                                     | Maximum points |
| ------------------------------------------------------------------------------------------- | -------------- |
| Sharing current developments with Judges - simulations, code, etc                           | 15             |
| Completing a test of a single subsystem (i.e. autonomous navigation, object detection, etc) | 15             |
| Recording useful data for further development                                               | 10             |
| General proficiency during the Test (i.e. being on time, having contingencies ready, etc)   | 10             |
| **Sum**                                                                                     | **50**         |

All of the subtasks will require some sort of proof that you've completed the tasks. Most of the methods from the Connectivity Test will be allowed. The exact possibilities will be provided during the Task. Time and language requirements are the same as for the Connectivity Test.

Useful data is defined as vaguely as possible - a set of notes or a video of the Task may be considered useful. Things like `rosbag` recordings are good candidates for a full score.

### Challenge Test

Challenge Test will be a test mocking the Final Challenge. The main difference will be that it still won't happen on the Marsyard, and that you'll be getting half as many points as in the Final Tasks. Time slot for a single Team will also be cut by half.

The Challenge Test may be streamed to all the Teams. Stream will most likely have a delay that'd make it impossible to use it for real-time control. The stream may be used for other Teams to get the same set of guidelines and advice from Judges.

## Final Challenge

Final Challenge will be held on the Marsyard, a special outdoor environment designed for testing rovers. The Marsyard is a large area with various obstacles and terrains. The Marsyard is not guaranteed to have the same shape or form to one on On-site formula.

### The story

Your mission was given a lander with two robots. One large, with a comprehensive set of sensors (Husarion Panther). The lander is equipped with some computing power too – so you can host your most time-sensitive algorithms there. The mission is to investigate a crash site of a previous expedition.

Your Task is to investigate the crash site using Husarion Panther and provide us with a report of what you’ve found.

### Ground rules

Detailed description of the environments will be provided in the Technical Handbook. In general the Task will take place on Marsyard - a special outdoor environment designed for testing rovers. The Marsyard is a large area with various obstacles and terrains. The Marsyard is not guaranteed to have the same shape or form to one on On-site formula.

During the Competition some Teams will inevitably make some risky moves and flip the robot or even let the robot loose and leave the designated area. In those, and in any case that may be a risk to both the equipment and the environment (including Judges, Volunteers and Spectators) Judges reserve a right to immediately interrupt the Task and take control of the Infrastructure. Exact repercussions will be decided by the Judges on a case-by-case basis, and may result in negative points or even disqualification. Judges may also decide to restore safety in any way they choose and let the Team continue. There will be no extra time granted for such interruptions. We do not want you to be overly cautious - but we do want you to be responsible - that's why will be awarding safety precautions with extra points - especially if nothing bad happens.

Keep in mind that these Challenges are not awarding being the fastest but being the most accurate. You will be given a certain time slot for interacting with the Judges and the environment but during that time you'll most likely be able to try some parts of the Tasks multiple times. We are aware that most of you will be competing from remote locations and we do not want to make it easier in any way for the Teams with a better Internet connection. We will not be compensating for a poor Internet connection quality either. Our goal for all Tasks is to provide challenges that are doable with a top score even with lags and a choppy streams. We do expect you to also take the remoteness into consideration and will be awarding being mindful of that (i.e. by running most of the software locally) as a part of Technical excellence scoring throughout the Competition.

Rules-wise - in case a Team has not finished given Task in a given time slot - the run is interrupted and scored as if it was a complete try. In cases of multiple attempts of allowed parts of the Tasks - scoring will be based on the best attempt. A Team can also end the Task earlier (and possibly be awarded some points for Social Excellence) or attempt any permitted by Rules part of the task again (and possibly have a better run).

Some parts of the scoring are being intentionally vague - this is to allow for the Teams to be creative and to show their skills. We do not want to limit you to a specific set of actions - we want you to show us what you're capable of. We will be awarding creativity and out-of-the-box thinking. Because of that, during the competition Jury will be making a list of ideas, used technologies, found objects etc - and then using it to to gauge what was possible and what was not. Max score for a given category will be then aligned close to the best result. Such lists (or parts of them) may be published after the Competition. This is also one of the reasons why you won't be given your score immediately after the Task - we want to holistically evaluate all the Teams to provide the scores.

Teams must communicate the technical (for the Technical excellence scoring category) and safety features (for the Safety considerations scoring category) for a particular run during the Task in order to get the points for those axes. Teams are encouraged to prepare a short description of the technical/safety features used beforehand. Such descriptions should be no longer than a single A4 page for each of the Challenges and should be provided to the Judges at the beginning of the Task. In case some mechanisms are disabled during the particular runs - such changes must be communicated to the Judges immediately.

### Final Challenge - Exploration Task - Investigate the crash site

During the Final Challenge on the Marsyard there will be AR tags scattered around the field, for you to use as a reference. Use of the tags is not required. There will also be Features - both things that **do** match the setting - like tools, rock formations, possibly other robots, and things that **do not** match the setting - like a piece of paper, a bottle of water, etc.

The goal is to investigate the crash site and provide a Report of what you've found. The Report should contain the information about the crash site, the objects found (including visual evidence like photos), whether they seem out of place and whether they seem to be useful during the remainder of the mission. Number of the Features won't be known before the Task. Numbers and locations of Features may change between the Teams.

The report should be human-readable and in a PDF format. The report should have a repeatable part, where each of the identified objects is described with a clear structure (things like - clear locations of photos, descriptions, etc). The report should be as comprehensive as possible. The report should be provided at most 15 minutes after the Task via the method specified by the Jury. Not providing the report in time may result in a zero-score for a Task.

Task can be done in a fully, partially or non-autonomous way. The Team must be able to provide a proof of any level of autonomy (and compliance with other rules) to the Judges during their assigned time slot.

Full autonomy requires no user input during the try. Remote monitoring by the Team during the autonomous run is acceptable. Partial autonomy is when the system requires some input from the user but there are large parts of it there are fully autonomous (i.e. driving and marking objects manually but identifying and generating the report automatically). All other cases are considered non-autonomous.

Multiple attempts are acceptable given the following conditions:
- no collected data is shared between the runs
- all tries are within the given time slot

Teams can, at any time resort to using lower level of autonomy, or not using the autonomy at all. Such switch must be communicated to Judges immediately. Lower levels of autonomy will be getting lower scores.

| Level of autonomy used | Challenge scoring multiplier |
| ---------------------- | ---------------------------- |
| Fully autonomous       | 100%                         |
| Partially autonomous   | 75%                          |
| Non-autonomous         | 50%                          |

Teams can and are encouraged to use the recent advances in machine learning, especially large language models (LLMs) for the Task. The use of external APIs is allowed. The use of LLMs and other machine learning features will be scored. Models running locally will be getting higher scores. Teams can use a mix of local and external models. Any fees for using the external services are the responsibility of the Team.

| Axis                                         | Maximum points |
| -------------------------------------------- | -------------- |
| Number of objects correctly identified       | 20             |
| Quality of visual evidence                   | 20             |
| Quality of the descriptions                  | 20             |
| Quality of reported locations                | 20             |
| Quality of report structure                  | 10             |
| Quality of report overview and summary       | 10             |
| Quality of an operator's panel and manual    | 20             |
| Returning the robot to the starting position | 10             |
| Technical excellence                         | 60             |
| Safety considerations                        | 10             |
| **Sum**                                      | **200**        |

**Technical excellence** - is a metric of the quality of used technical solutions - things like, but not limited to: whether machine learning models are used, how many aids/sensors are used to achieve the task, how reliably the algorithms were, etc.

**Safety considerations** - is a metric of the quality of safety features implemented - things like, but not limited to: whether there were any mechanisms providing auto-shutdown, whether there was a remote-kill switch, whether there were any sanity checks on the sensors and behavior of the robot, etc.

> [!NOTE]
> Points will be deducted for driving over the Landmarks, Markers (as described in the Technical Handbook) and so on. Depending on the severity of the violation - points may be deducted from the Technical excellence, Safety considerations or even the Jury points. The exact number of points deducted will be decided by the Judges on a case-by-case basis.

## Other activities

### Competition Report

Competition Report is a document (ideally a PDF file) that should contain the summary of the Competition from the Team's point of view - think of it as an Agile Retrospective for both the Team's performance and the Competition itself. It can also contain verbose feedback for the Operator. The Competition Report should be submitted at most one day after the Finals. The Competition Report should not contain any sensitive information. The Competition Report must not contain any personal information other than names and surnames. The Competition Report is subject to scoring. The Competition Reports may get published after the Competition. Competition Reports are not meant to be submitted early - it should be as close to the last interaction with Competition as possible and should contain the feedback on the whole Competition. Competition Reports should be reasonably short and concise. The maximum length of the Competition Report is 5 A4 pages (not including the pictures, diagrams, and other non-text elements). The font size should be at least 10pt. Teams are encouraged to do internal retrospectives during the Competition and use the results in the Competition Report. Competition Report can be composed of a set of such smaller retrospective reports.

| Area                   | Maximum points |
| ---------------------- | -------------- |
| Team's performance     | 40             |
| Operator's performance | 40             |
| **Sum**                | **80**         |

Points will be awarded based on the number of axes covered, the quality of the feedback (i.e. whether the point can be easily understood), the actionability of the feedback, etc. Feedback should contain both the negative and the positive points. For the sake of proving the number of axes considered, feedback can include neutral points. Points will not be deducted for any given point.

### Social Excellence

Social Excellence points are a living rank of current Teams' activity on a given axis. The Jury will be updating the points at their own discretion. The Social Excellence points are not limited to the technical aspects of the Competition. Each Social Excellence point increase or decrease does not have to have any explanation. The Social Excellence points are not limited to the Competition itself - they may also be granted for the Team's behavior outside of the Competition. Social Excellence points can be negative. Negative points are to be used in case of the Team's violation of the Rules, fair play or Social Excellence that are not covered by the disqualification.

| Area                 | Maximum points |
| -------------------- | -------------- |
| Community Forum      | 50             |
| Social Media         | 30             |
| Sponsor interactions | 10             |
| Community outreach   | 10             |
| **Sum**              | **100**        |

**Community Forum** - points will be awarded for positive interactions on the Community Forum. This may include answering questions, providing feedback, sharing knowledge, etc. Points may be awarded for the number of interactions, the quality of interactions, the helpfulness of interactions, etc. Sharing possible solutions/algorithms/code or fixing issues in materials from other Teams would be considered the highest form of interaction. Points may be deducted for negative interactions - like but not limited to: asking the same questions multiple times, not looking for the answer in the materials provided, providing misleading information, providing unrelated discussions, etc.

**Social Media/marketing** - Teams are expected to have some social media presence. Publicly sharing progress (both the technical one and organizational one) is encouraged. Points will be awarded for the number of posts, the quality of posts, the reach of posts, etc. Points may be deducted for negative posts. Social Media platforms are not limited to any specific ones - but the most common ones are expected to be used. In order to get the points for the Social Media - the posts should be publicly available and scored profiles must be known to the Operator (i.e. provided in a Registration Form or in a Team-specific post on Community Forum). Writing a post on a Community Forum is not considered a Social Media post. Social Media presence is not required to be infinitely professional and pristine - it's meant to be engaging to the desired audiences. Things like the number of interactions and regularity will be taken into the account too.

**Sponsor interactions** - Teams are not required to have any sponsors but are encouraged to have some. Points will be awarded for successful sponsorships/mentorships and for seeking sponsors/mentors. In order to qualify for the points - the sponsor/mentor must be known to the Operator - ideally listed on a Team-specific post on the Community Forum (if possible) or sent in a direct communication with the Operator.

**Community outreach** - points will be awarded for the Team's activity with the local community outside of the Competition. This may include but is not limited to: organizing workshops, presentations, visiting schools, hackerspaces and so on. In order to qualify for the points - the activity must be known to the Operator - ideally listed on social media, a Team-specific post on the Community Forum (if possible) or sent in a direct communication with the Operator.

### Jury points

Organizer is entitled to grant points to the Teams at their own discretion at any point in time. The Jury points are not final and may get updated at any time. The Jury points are to be used only in case of extraordinary situations. The Jury points are not limited to the technical aspects of the Competition. Each Jury points increase or decrease should have an explanation provided by the Operator. The Jury points are not limited to the Competition itself - they may also be granted for the Team's behavior outside of the Competition. Jury points can be negative. Negative points are to be used in case of the Team's violation of the Rules, fair play or Social Excellence that are not covered by the disqualification.

## Award ceremony and final scores

Award Ceremony will be held together with the On-site formula. In-person participation is not required. In-person participation requires coordination with the Organizer. The number of seats during the Ceremony is limited.

Final scores will be published online on the day of the Ceremony. The final scores are not negotiable after the Ceremony. Feedback about the final scoring can be provided to the Operator during the first two weeks after the Ceremony and may be considered for the next year's Competition.

## Final words

We sincerely hope that everyone will learn a lot during this Competition and that the Competition will be a fun and engaging experience for all the Teams. We are looking forward to seeing your proposals and your progress during the Competition. We wish you good luck and a lot of fun during the Competition.
